# Run:
```bash
source /workspace/conda/etc/profile.d/conda.sh
conda activate musetalk

export CUDA_VISIBLE_DEVICES=0    # optional, if single GPU box
uvicorn app:app --host 0.0.0.0 --port 8000 --workers 1
```

# Note:
> If you want NVENC for faster muxing, change the ffmpeg mux command to:
```bash
ffmpeg -y -loglevel error -i "{tmp_video}" -i "{audio_path}" -c:v copy -c:a aac -shortest "{final_video}"
```

* (We already avoid re-encoding video; MuseTalk frames are encoded via OpenCV libx264 in tmp_video and we just copy video stream into final file.)
* If you later decide to pipe frames to ffmpeg (no temp file), I can swap the writer for a `subprocess.Popen(...)`pipe.
* If you want batch_size tuning, set it in your YAMLs; the loader merges YAML into defaults.
* The wrapper uses your existing utilities (preprocessing, blending, datagen, VAE/UNet loaders, Whisper feature chunking), so behavior matches your repo.
